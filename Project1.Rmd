---
title: "CSCI 550 Project 1"
author: "Heather Koyuk, Muzhou Chen$, Greg Martin"
date: "9/27/2022"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages

```{r load-packages, message=FALSE}
library(tidyverse)
library(knitr)
library(lubridate)
library(psych)
```

```{r load-data, message=FALSE}
Customer_telecom <- read.csv("Customer_telecom.csv")
                             
```

## TODO: Executive Summary


## EDA

```{r eda}

num_c <- Customer_telecom %>%
  dplyr::select(account.length, number.vmail.messages, total.day.minutes, total.day.calls, total.day.charge, total.eve.minutes, total.eve.calls, total.eve.charge, total.night.minutes, total.night.calls, total.night.charge, total.intl.minutes, total.intl.calls, total.intl.charge, customer.service.calls)

cor(num_c)
summary(Customer_telecom)

num_c <- num_c %>%
  dplyr::select(account.length, number.vmail.messages, total.day.minutes, total.day.calls, total.eve.minutes, total.eve.calls, total.night.minutes, total.night.calls, total.intl.minutes, total.intl.calls, customer.service.calls)

minutes_c <- num_c %>%
  dplyr::select(account.length, number.vmail.messages, total.day.minutes, total.eve.minutes, total.night.minutes, total.intl.minutes, customer.service.calls)

pairs(minutes_c)

calls_c <-  Customer_telecom %>%
  dplyr::select(account.length, number.vmail.messages, total.day.calls, total.eve.calls, total.night.calls, total.intl.calls, customer.service.calls)
pairs(calls_c)

sums_c <- Customer_telecom %>%
  mutate(total_minutes = sum(total.day.minutes, total.eve.minutes, total.night.minutes, total.intl.minutes)) %>%
  mutate(total_calls = sum(total.day.calls, total.eve.calls, total.night.calls, total.intl.calls)) %>%
  dplyr::select(account.length, number.vmail.messages, total_minutes, total_calls,  customer.service.calls)
                             
pairs(sums_c)

Customer_telecom <- Customer_telecom %>%
  mutate(churn1 = if_else(churn == "False", 0, 1))

glm.fits <- glm(
  churn1 ~ total.day.minutes,
  data = Customer_telecom, family = binomial
)
summary(glm.fits)

glm.fits1 <- glm(
  churn1 ~ account.length+ number.vmail.messages+ total.day.minutes+total.day.calls+ total.day.charge+ total.eve.minutes+ total.eve.calls+total.eve.charge+ total.night.minutes+total.night.calls+ total.night.charge+ total.intl.minutes+total.intl.calls+total.intl.charge+ customer.service.calls,
  data = Customer_telecom, family = binomial
)
summary(glm.fits1)
```
Based on the result from our preprocess, the output from cor() shows all correlations between two variables, the two variables with high correlation will not be in the same model. The LRM glm.fits1 including all predictors provides a summary of z-Score and p-value. We will start fitting models with the predictors number.vmail.messages(p=4.89e-08), total.intl.calls(p=0.00077), and customer.service.calls(p=< 2e-16).

## Models

##LDA
```{r lda}
library(MASS)
library(caret)


#Randomly shuffle the data
Customer_telecom<-Customer_telecom[sample(nrow(Customer_telecom)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(Customer_telecom)),breaks=10,labels=FALSE)

totalMisclassificationRate=c()
runningTotal=c()

#K-fold Cross Validation
for (i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  test_data <- Customer_telecom[testIndexes, ]
  train_data <- Customer_telecom[-testIndexes, ]
  m_lda <- lda(churn1 ~ number.vmail.messages + total.intl.calls + customer.service.calls, data = train_data)
  m_pred <- predict(m_lda, test_data)
  conf <- table(list(predicted=m_pred$class, observed=test_data$churn1))
  confusionMatrix(conf)
  print(conf)
  totalPreds = sum(conf)
  incorrectPreds <- conf[1,2] + conf[2,1]
  misclassificationRate <- incorrectPreds / totalPreds
  print(misclassificationRate)
  runningTotal <- c(runningTotal, misclassificationRate)
}

totalMisclassificationRate=c(totalMisclassificationRate, mean(runningTotal))
print("Average Misclassification Rate")
totalMisclassificationRate
```

##QDA
```{r qda}
library(MASS)

#Randomly shuffle the data
Customer_telecom<-Customer_telecom[sample(nrow(Customer_telecom)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(Customer_telecom)),breaks=10,labels=FALSE)

totalMisclassificationRate <- c()
runningTotal <- c()

for (i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  test_data <- Customer_telecom[testIndexes, ]
  train_data <- Customer_telecom[-testIndexes, ]
  m_qda <- qda(churn1 ~ number.vmail.messages + total.intl.calls + customer.service.calls, data = train_data)
  m_pred <- predict(m_qda, test_data)
  conf <- table(list(predicted=m_pred$class, observed=test_data$churn1))
  confusionMatrix(conf)
  print(conf)
  totalPreds = sum(conf)
  incorrectPreds <- conf[1,2] + conf[2,1]
  misclassificationRate <- incorrectPreds / totalPreds
  print(misclassificationRate)
  runningTotal <- c(runningTotal, misclassificationRate)
}

totalMisclassificationRate=c(totalMisclassificationRate, mean(runningTotal))
print("Average Misclassification Rate")
totalMisclassificationRate

```


##Naive Bayes

```{r naive}
library(MASS)
library(e1071)

#Randomly shuffle the data
Customer_telecom<-Customer_telecom[sample(nrow(Customer_telecom)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(Customer_telecom)),breaks=10,labels=FALSE)

totalMisclassificationRate <- c()
runningTotal <- c()

for (i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  test_data <- Customer_telecom[testIndexes, ]
  train_data <- Customer_telecom[-testIndexes, ]
  m_naive <- naiveBayes(churn1 ~ number.vmail.messages + total.intl.calls + customer.service.calls, data = train_data)
  m_pred <- predict(m_naive, test_data)
  
  conf <- table(list(predicted=m_pred, observed=test_data$churn1))
  confusionMatrix(conf)
  print(conf)
  totalPreds = sum(conf)
  incorrectPreds <- conf[1,2] + conf[2,1]
  misclassificationRate <- incorrectPreds / totalPreds
  print(misclassificationRate)
  runningTotal <- c(runningTotal, misclassificationRate)
}

totalMisclassificationRate=c(totalMisclassificationRate, mean(runningTotal))
print("Average Misclassification Rate")
totalMisclassificationRate


```

##LDA Experiments

##LDA with next most significant predictor

```{r lda}
library(MASS)
library(caret)


#Randomly shuffle the data
Customer_telecom<-Customer_telecom[sample(nrow(Customer_telecom)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(Customer_telecom)),breaks=10,labels=FALSE)

totalMisclassificationRate=c()
runningTotal=c()

#K-fold Cross Validation
for (i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  test_data <- Customer_telecom[testIndexes, ]
  train_data <- Customer_telecom[-testIndexes, ]
  m_lda2 <- lda(churn1 ~ number.vmail.messages + total.intl.calls + customer.service.calls + total.day.calls, data = train_data)
  m_pred <- predict(m_lda2, test_data)
  conf <- table(list(predicted=m_pred$class, observed=test_data$churn1))
  confusionMatrix(conf)
  print(conf)
  totalPreds = sum(conf)
  incorrectPreds <- conf[1,2] + conf[2,1]
  misclassificationRate <- incorrectPreds / totalPreds
  print(misclassificationRate)
  runningTotal <- c(runningTotal, misclassificationRate)
}

totalMisclassificationRate=c(totalMisclassificationRate, mean(runningTotal))
print("Average Misclassification Rate")
totalMisclassificationRate
```

By adding one more predictor total.day.calls(the potential predictor with the fourth smallest p-value from LRM), the average misclassification rate tend to increase slightly, which may be caused by overfitting. --So we stop by including 3 predictors in our model. 

##LDA All potentially relevant predictors
```{r lda}
library(MASS)
library(caret)


#Randomly shuffle the data
Customer_telecom<-Customer_telecom[sample(nrow(Customer_telecom)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(Customer_telecom)),breaks=10,labels=FALSE)

totalMisclassificationRate=c()
runningTotal=c()

#K-fold Cross Validation
for (i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  test_data <- Customer_telecom[testIndexes, ]
  train_data <- Customer_telecom[-testIndexes, ]
  m_lda2 <- lda(churn1 ~ state + account.length + international.plan + voice.mail.plan + number.vmail.messages + total.day.minutes + total.eve.minutes + total.night.minutes + total.intl.minutes + customer.service.calls, data = train_data)
  m_pred <- predict(m_lda2, test_data)
  conf <- table(list(predicted=m_pred$class, observed=test_data$churn1))
  confusionMatrix(conf)
  print(conf)
  totalPreds = sum(conf)
  incorrectPreds <- conf[1,2] + conf[2,1]
  misclassificationRate <- incorrectPreds / totalPreds
  print(misclassificationRate)
  runningTotal <- c(runningTotal, misclassificationRate)
}

totalMisclassificationRate=c(totalMisclassificationRate, mean(runningTotal))
print("Average Misclassification Rate")
totalMisclassificationRate
```

Using all predictors except those that would not be meaningful (such as phone number), we get an average misclassification rate that is higher than our selected predictors, this is likely an indicator that the model is capturing unimportant observations.

##LDA All call metrics
```{r lda}
library(MASS)
library(caret)


#Randomly shuffle the data
Customer_telecom<-Customer_telecom[sample(nrow(Customer_telecom)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(Customer_telecom)),breaks=10,labels=FALSE)

totalMisclassificationRate=c()
runningTotal=c()

#K-fold Cross Validation
for (i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  test_data <- Customer_telecom[testIndexes, ]
  train_data <- Customer_telecom[-testIndexes, ]
  m_lda2 <- lda(churn1 ~ number.vmail.messages + total.intl.calls + customer.service.calls + total.day.calls + total.night.calls, data = train_data)
  m_pred <- predict(m_lda2, test_data)
  conf <- table(list(predicted=m_pred$class, observed=test_data$churn1))
  confusionMatrix(conf)
  print(conf)
  totalPreds = sum(conf)
  incorrectPreds <- conf[1,2] + conf[2,1]
  misclassificationRate <- incorrectPreds / totalPreds
  print(misclassificationRate)
  runningTotal <- c(runningTotal, misclassificationRate)
}

totalMisclassificationRate=c(totalMisclassificationRate, mean(runningTotal))
print("Average Misclassification Rate")
totalMisclassificationRate
```

Using the top predictors associated with call metrics, we get a very similar rate as with the top three predictors. This likely means that the significance of the additional predictors is already captured in the top three predictors.